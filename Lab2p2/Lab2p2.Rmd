---
title: "Lab2p2"
output: html_document
date: "2024-10-04"
---

```{r}
# library("e1071")
# 
# #insta
# 
# iris
# 
# iris.df <- iris
# #?naiveBayes
# 
# classifier <- naiveBayes(iris[,1:4], iris[,5])
# 
# classifier
# 
# prediction <- predict(classifier, iris[,-5])
# 
# prediction
# 
# table(prediction, iris[,5], dnn=list('predicted','actual'))
# 
# classifier$apriori
# 
# classifier$tables$Petal.Length
# 
# plot(function(x) dnorm(x, 1.462, 0.1736640), 0, 8, col="red", main="Petal length distribution for the 3 different species") 
# 
# curve(dnorm(x, 4.260, 0.4699110), add=TRUE, col="blue") 
# 
# curve(dnorm(x, 5.552, 0.5518947 ), add=TRUE, col = "green")
# 
# library(rpart)
# 
# library(rpart.plot)
# 
# dim(iris)
# 
# s_iris <- sample(150,100)
# 
# s_iris
# 
# iris_train <-iris[s_iris,]
# 
# iris_test <-iris[-s_iris,] 
# 
# dim(iris_test)
# dim(iris_train) 
# 
# # ?rpart
# 
# dectionTreeModel <- rpart(Species~., iris_train, method = "class") 
# 
# dectionTreeModel
# 
# rpart.plot(dectionTreeModel) 
```

Exercise 1:
• Repeat the naïve bayes analysis using the abalone dataset.
• Try 3 different subsets of features not just all features at once.
• Compare models using contingency tables.
• Plot the distribution of classes along 3 different features.
```{r}
### Abelone ####

abalone <- read.csv("abalone.data", header = FALSE, sep = ",")

colnames(abalone) <- c("sex", "length", 'diameter', 'height', 'whole_weight', 'shucked_wieght', 'viscera_wieght', 'shell_weight', 'rings' )

abalone$age.group <- cut(abalone$rings, br=c(0,8,11,35), labels = c("young", 'adult', 'old')) 

abalone.norm <- abalone[,-1]
#####################
classifier <- naiveBayes(abalone[,2:4], abalone[,10])

classifier

prediction <- predict(classifier, abalone[,2:4])

# prediction

table(prediction, abalone[,10], dnn=list('predicted','actual'))

# classifier$apriori

classifier$tables$length
plot(function(x) dnorm(x, 0.4209915, 0.11137474), xlim=c(0, 1), ylim=c(0,5), col="red", main="Abalone length distribution [red=young, blue=adult, green=old]")
curve(dnorm(x, 0.5707182, 0.08740980), add=TRUE, col="blue") 
curve(dnorm(x, 0.5868542, 0.08100644), add=TRUE, col = "green")

classifier$tables$diameter
plot(function(x) dnorm(x, 0.3212758, 0.09029187), xlim=c(0, .8), ylim=c(0,7), col="red", main="Abalone diameter distribution [red=young, blue=adult, green=old]")
curve(dnorm(x, 0.4458591, 0.07153798), add=TRUE, col="blue") 
curve(dnorm(x, 0.4632083, 0.06699741), add=TRUE, col = "green")

classifier$tables$height
plot(function(x) dnorm(x, 0.1065956, 0.04183039), xlim=c(0, .3), ylim=c(0,15), col="red", main="Abalone diameter distribution [red=young, blue=adult, green=old]")
curve(dnorm(x, 0.1516906, 0.02984784), add=TRUE, col="blue") 
curve(dnorm(x, 0.1648125, 0.02935998), add=TRUE, col = "green")
######################
classifier2 <- naiveBayes(abalone[,4:6], abalone[,10])

classifier2

prediction2 <- predict(classifier2, abalone[,4:6])

# prediction2

table(prediction2, abalone[,10], dnn=list('predicted','actual'))
######################
classifier3 <- naiveBayes(abalone[,6:8], abalone[,10])

classifier3

prediction3 <- predict(classifier3, abalone[,6:8])

# prediction3

table(prediction3, abalone[,10], dnn=list('predicted','actual'))
```

Exercise 2:
• Repeat the kNN analysis using the iris dataset.
• Try 2 different subsets of features.
• Compare models using contingency tables and accuracy plots.
```{r}
iris <- read.csv("iris.csv", header = FALSE, sep = ",")

colnames(iris) <- c("id", "Sepal.Length", "Sepal.Width", 'Petal.Length', 'Petal.Width', 'Species')

# drop the sex column (categorical variable)
iris.norm <- iris[-1,-1]
# optionally normalize
iris.norm
s_iris <- sample(151,100)

## create train & test sets based on sampled indexes
iris.train <-iris.norm[s_iris,]
iris.test <-iris.norm[-s_iris,]

iris.train <- iris.train[complete.cases(iris.train),]
iris.test <- iris.test[complete.cases(iris.test),]
# train model & predict
KNNpred <- knn(train = iris.train[1:2], test = iris.test[1:2], cl = iris.train$Species, k = 55)
# create contingency table/ confusion matrix
contingency.table <- table(KNNpred,iris.test$Species)

contingency.table

contingency.matrix = as.matrix(contingency.table)
sum(diag(contingency.matrix))/length(iris.test$Species)
accuracy <- c()
ks <- c(35,45,55,65,75,85,95)
for (k in ks) {
KNNpred <- knn(train = iris.train[1:2], test = iris.test[1:2], cl = iris.train$Species, k = k)
cm = as.matrix(table(Actual=KNNpred, Predicted = iris.test$Species, dnn=list('predicted','actual')))

accuracy <- c(accuracy,sum(diag(cm))/length(iris.test$Species))
}
plot(ks,accuracy,type = "b")
###############################3

KNNpred <- knn(train = iris.train[2:4], test = iris.test[2:4], cl = iris.train$Species, k = 55)
# create contingency table/ confusion matrix
contingency.table <- table(KNNpred,iris.test$Species)

contingency.table

contingency.matrix = as.matrix(contingency.table)
sum(diag(contingency.matrix))/length(iris.test$Species)
accuracy <- c()
ks <- c(35,45,55,65,75,85,95)
for (k in ks) {
KNNpred <- knn(train = iris.train[2:4], test = iris.test[2:4], cl = iris.train$Species, k = k)
cm = as.matrix(table(Actual=KNNpred, Predicted = iris.test$Species, dnn=list('predicted','actual')))

accuracy <- c(accuracy,sum(diag(cm))/length(iris.test$Species))
}
plot(ks,accuracy,type = "b")
```


Exercise 3:
• Run k-means analysis using the abalone & iris datasets.
• Try different values of k for both.
• Evaluate clustering using Plot the best clustering output for both.
```{r}
iris <- read.csv("iris.csv", header = FALSE, sep = ",")
attach(iris)
iris

colnames(iris) <- c("id", "Sepal.Length", "Sepal.Width", 'Petal.Length', 'Petal.Width', 'Species')
iris <- iris[-1,]
iris

# Plot iris petal length vs. petal width, color by species
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
geom_point()
# set seed for random number generator
set.seed(123)
# run k-means
iris.km <- kmeans(iris[,-6], centers = 3)
assigned.clusters <- as.factor(iris.km$cluster)
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = assigned.clusters)) +
geom_point()

wss <- c()
ks <- c(1,5,7,9)
for (k in ks) {
iris.km <- kmeans(iris[,-6], centers = k)
wss <- c(wss,iris.km$tot.withinss)
}
plot(ks,wss,type = "b")

labeled.clusters <- as.character(assigned.clusters)
labeled.clusters[labeled.clusters==1] <- "setosa"
labeled.clusters[labeled.clusters==2] <- "versivolor"
labeled.clusters[labeled.clusters==3] <- "virginica"
table(labeled.clusters, iris[,6])


abalone <- read.csv("abalone.data", header = FALSE, sep = ",")

colnames(abalone) <- c("sex", "length", 'diameter', 'height', 'whole_weight', 'shucked_wieght', 'viscera_wieght', 'shell_weight', 'rings' )

abalone$age.group <- cut(abalone$rings, br=c(0,8,11,35), labels = c("young", 'adult', 'old'))

abalone <- abalone[,-1]

ggplot(abalone, aes(x = length, y = diameter, colour = age.group)) +
geom_point()
# set seed for random number generator
set.seed(123)
# run k-means
abalone.km <- kmeans(abalone[,-9], centers = 3)
assigned.clusters <- as.factor(abalone.km$cluster)
ggplot(abalone, aes(x = length, y = diameter, colour = assigned.clusters)) +
geom_point()

wss <- c()
ks <- c(1,5,7,9)
for (k in ks) {
abalone.km <- kmeans(abalone[,-9], centers = k)
wss <- c(wss,abalone.km$tot.withinss)
}
plot(ks,wss,type = "b")

labeled.clusters <- as.character(assigned.clusters)
labeled.clusters[labeled.clusters==1] <- "young"
labeled.clusters[labeled.clusters==2] <- "adult"
labeled.clusters[labeled.clusters==3] <- "old"
table(labeled.clusters, abalone[,9])
```

