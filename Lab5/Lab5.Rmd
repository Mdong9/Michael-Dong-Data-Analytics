---
title: "Lab5"
author: "Michael Dong"
date: "2024-11-15"
output: html_document
---

```{r}

library("caret")
library(e1071)
library(class)

setwd(getwd())
wine_data <- read.csv("./wine/wine.data", header = FALSE, sep = ",")      #wine attribute data
wine_colnames <- read.csv("./wine/wine.names", header = FALSE, sep = ",") #wine column names
colnames(wine_data) <- c("Type", wine_colnames[48:60,1])                  #labeling wine columns\

wine_data$Type <- as.factor(wine_data$Type)

set.seed(123)
wine.indexes <- sample(178, 0.7*178)


wine_train <- wine_data[wine.indexes,c(1,2,3,7)]
wine_test <- wine_data[-wine.indexes,c(1,2,3,7)]
```


Linear Kernel SVM Model
```{r}
## train SVM model - linear kernel
svm.mod0 <- svm(wine_train$Type ~ ., data = wine_train, kernel = 'linear')

svm.mod0

wine_train.pred <- predict(svm.mod0, wine_train)

cm = as.matrix(table(Actual = wine_train$Type, Predicted = wine_train.pred))

cm

n = sum(cm) # number of instances
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class
rowsums = apply(cm, 1, sum) # number of instances per class
colsums = apply(cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted

recall = diag / rowsums
precision = diag / colsums
f1 = 2 * precision * recall / (precision + recall)

data.frame(precision, recall, f1)
```


Polynomial Kernel SVM Model
```{r}
## train SVM model - polynomial kernel
svm.mod1 <- svm(wine_train$Type ~ ., data = wine_train, kernel = 'polynomial')

svm.mod1

wine_train.pred <- predict(svm.mod1, wine_train)

cm = as.matrix(table(Actual = wine_train$Type, Predicted = wine_train.pred))

cm

n = sum(cm) # number of instances
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class 
rowsums = apply(cm, 1, sum) # number of instances per class
colsums = apply(cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted 

recall = diag / rowsums 
precision = diag / colsums
f1 = 2 * precision * recall / (precision + recall) 

data.frame(precision, recall, f1)
```


KNN model (wines)
```{r}
KNN_wine <- knn(train = wine_train, test = wine_test, cl = wine_train$Type, k = 5)

cm = as.matrix(table(Actual=wine_test$Type, Predicted = KNN_wine, dnn=list('actual','predicted')))

cm

n = sum(cm) # number of instances
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class 
rowsums = apply(cm, 1, sum) # number of instances per class
colsums = apply(cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted 

recall = diag / rowsums 
precision = diag / colsums
f1 = 2 * precision * recall / (precision + recall) 

data.frame(precision, recall, f1)
```



SVM Regression Model
```{r}
NY <- read.csv("NY-House-Dataset.csv", header = TRUE, sep = ",")
svm.mod2 <- svm(log10(PRICE) ~ log10(PROPERTYSQFT), data = NY, kernel = 'linear')

svm.mod2

train.pred <- predict(svm.mod2, NY)

ggplot(NY, aes(x = train.pred, y = log10(PRICE))) +
  geom_point()

```


Linear Regression Model
```{r}
NY <- read.csv("NY-House-Dataset.csv", header = TRUE, sep = ",")
attach(NY)
NY <- NY[,c(3,6,7)]
ny.lin.mod <- lm(log10(PRICE) ~ log10(PROPERTYSQFT), NY)

plot(log10(PRICE) ~ log10(PROPERTYSQFT))

lm_predict <- predict(ny.lin.mod,NY)

ggplot(NY, aes(x = lm_predict, y = log10(PRICE))) +
  geom_point()
```

